{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1b2c3d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T19:06:19.429069Z",
     "iopub.status.busy": "2026-02-22T19:06:19.428065Z",
     "iopub.status.idle": "2026-02-22T19:08:24.680218Z",
     "shell.execute_reply": "2026-02-22T19:08:24.680218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 540 official cards from dimlas5_cardlist.csv\n",
      "Initializing EasyOCR with GPU...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EasyOCR ready!\n",
      "\n",
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=998983614815-vbmp7vtvst82rr581h8kon2egjaqli54.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A59075%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.readonly&state=Lsx8k7T3azyJYrZHdodJDCieCWgemC&access_type=offline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Locating newest draft on Google Drive...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Season  : Season 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Draft   : 20260222 Draft 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Players : 12 image(s) found\n",
      "\n",
      "Raw CSVs     : c:\\Users\\Dimlas\\Desktop\\Dimi\\Github\\CubeOCR\\data\\drafted_decks\\20260222_Draft_8\n",
      "Detailed CSVs: c:\\Users\\Dimlas\\Desktop\\Dimi\\Github\\CubeOCR\\data\\drafted_decks\\20260222_Draft_8\\detailed OCR\n",
      "Clean CSVs   : c:\\Users\\Dimlas\\Desktop\\Dimi\\Github\\CubeOCR\\data\\clean\\20260222_Draft_8\n",
      "Clean images : c:\\Users\\Dimlas\\Desktop\\Dimi\\Github\\CubeOCR\\data\\clean\\20260222_Draft_8\\clean images\n"
     ]
    }
   ],
   "source": [
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import glob\n",
    "import numpy as np\n",
    "import easyocr\n",
    "from PIL import Image, ImageDraw\n",
    "import io\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# Project root is one level up from this scripts/ folder\n",
    "PROJECT_ROOT      = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "from config import SCOPES, MAIN_FOLDER_ID\n",
    "\n",
    "DRAFTED_DECKS_DIR = os.path.join(PROJECT_ROOT, 'data', 'drafted_decks')\n",
    "CLEAN_OUTPUT_DIR  = os.path.join(PROJECT_ROOT, 'data', 'clean')\n",
    "CARDLIST_DIR      = os.path.join(PROJECT_ROOT, 'data', 'cardlist')\n",
    "\n",
    "SIMILARITY_THRESHOLD = 0.65\n",
    "\n",
    "# --- Load official card list ---\n",
    "cube_lists = glob.glob(os.path.join(CARDLIST_DIR, 'dimlas*_cardlist.csv'))\n",
    "if not cube_lists:\n",
    "    raise FileNotFoundError(f'No cube list found in {CARDLIST_DIR}')\n",
    "CUBE_LIST_FILE = sorted(\n",
    "    cube_lists,\n",
    "    key=lambda f: int(re.search(r'dimlas(\\d+)_cardlist', f).group(1)),\n",
    "    reverse=True\n",
    ")[0]\n",
    "\n",
    "official_cards = set()\n",
    "name_to_scryfall_id = {}\n",
    "with open(CUBE_LIST_FILE, 'r', encoding='utf-8', newline='') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    name_col     = next((c for c in reader.fieldnames if c.strip().lower() == 'name'), None)\n",
    "    scryfall_col = next((c for c in reader.fieldnames if c.strip().lower() == 'scryfall_id'), None)\n",
    "    for row in reader:\n",
    "        card = row[name_col].strip()\n",
    "        if card:\n",
    "            official_cards.add(card)\n",
    "            if scryfall_col:\n",
    "                name_to_scryfall_id[card] = row[scryfall_col].strip()\n",
    "official_cards_lower = {c.lower(): c for c in official_cards}\n",
    "print(f'Loaded {len(official_cards)} official cards from {os.path.basename(CUBE_LIST_FILE)}')\n",
    "\n",
    "# --- Initialize EasyOCR ---\n",
    "print('Initializing EasyOCR with GPU...')\n",
    "reader_ocr = easyocr.Reader(['en'], gpu=True)\n",
    "print('EasyOCR ready!\\n')\n",
    "\n",
    "# --- Google Drive auth ---\n",
    "TOKEN_PATH       = os.path.join(PROJECT_ROOT, 'token.json')\n",
    "CREDENTIALS_PATH = os.path.join(PROJECT_ROOT, 'credentials.json')\n",
    "if os.path.exists(TOKEN_PATH):\n",
    "    creds = Credentials.from_authorized_user_file(TOKEN_PATH, SCOPES)\n",
    "else:\n",
    "    flow = InstalledAppFlow.from_client_secrets_file(CREDENTIALS_PATH, SCOPES)\n",
    "    creds = flow.run_local_server(port=0)\n",
    "    with open(TOKEN_PATH, 'w') as token:\n",
    "        token.write(creds.to_json())\n",
    "drive_service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "# --- Drive helpers ---\n",
    "def get_folders(parent_id, name_pattern=None):\n",
    "    \"\"\"Get non-trashed folders from a parent folder.\"\"\"\n",
    "    query   = f\"'{parent_id}' in parents and mimeType='application/vnd.google-apps.folder' and trashed=false\"\n",
    "    folders = drive_service.files().list(q=query, fields='files(id, name)').execute().get('files', [])\n",
    "    if name_pattern:\n",
    "        folders = [f for f in folders if re.search(name_pattern, f['name'])]\n",
    "    return folders\n",
    "\n",
    "def get_files(parent_id, mime_type_filter=None):\n",
    "    \"\"\"Get non-trashed files from a folder, sorted by name.\"\"\"\n",
    "    query = f\"'{parent_id}' in parents and trashed=false\"\n",
    "    if mime_type_filter:\n",
    "        query += f\" and mimeType contains '{mime_type_filter}'\"\n",
    "    files = drive_service.files().list(q=query, fields='files(id, name, mimeType)').execute().get('files', [])\n",
    "    return sorted(files, key=lambda x: x['name'])\n",
    "\n",
    "def is_player_file(filename):\n",
    "    \"\"\"Check if file is a player file (not an overview/backup file).\"\"\"\n",
    "    name_lower = filename.lower()\n",
    "    if '+' in filename:            return False\n",
    "    if 'result'   in name_lower:   return False\n",
    "    if 'standing' in name_lower:   return False\n",
    "    if re.search(r'^r\\d', name_lower): return False\n",
    "    return True\n",
    "\n",
    "def download_image(file_id):\n",
    "    \"\"\"Download image file from Google Drive.\"\"\"\n",
    "    return drive_service.files().get_media(fileId=file_id).execute()\n",
    "\n",
    "# --- Find newest draft on Drive ---\n",
    "print('Locating newest draft on Google Drive...')\n",
    "season_folders = get_folders(MAIN_FOLDER_ID, r'Season \\d+')\n",
    "newest_season  = max(season_folders, key=lambda f: int(re.search(r'Season (\\d+)', f['name']).group(1)))\n",
    "print(f'  Season  : {newest_season[\"name\"]}')\n",
    "\n",
    "folders_in_season = get_folders(newest_season['id'])\n",
    "pictures_folder   = next(f for f in folders_in_season if f['name'].lower() == 'pictures')\n",
    "\n",
    "draft_folders = get_folders(pictures_folder['id'], r'\\d{8}\\s+Draft\\s+\\d+')\n",
    "newest_draft  = max(draft_folders, key=lambda f: int(re.match(r'(\\d{8})', f['name']).group(1)))\n",
    "print(f'  Draft   : {newest_draft[\"name\"]}')\n",
    "\n",
    "all_files    = get_files(newest_draft['id'], mime_type_filter='image/')\n",
    "player_files = [f for f in all_files if is_player_file(f['name'])]\n",
    "print(f'  Players : {len(player_files)} image(s) found\\n')\n",
    "\n",
    "# --- Create output directories ---\n",
    "draft_name    = newest_draft['name'].replace(' ', '_')\n",
    "output_dir    = os.path.join(DRAFTED_DECKS_DIR, draft_name)\n",
    "detailed_dir  = os.path.join(output_dir, 'detailed OCR')\n",
    "clean_dir     = os.path.join(CLEAN_OUTPUT_DIR, draft_name)\n",
    "clean_img_dir = os.path.join(clean_dir, 'clean images')\n",
    "for d in [output_dir, detailed_dir, clean_dir, clean_img_dir]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(f'Raw CSVs     : {output_dir}')\n",
    "print(f'Detailed CSVs: {detailed_dir}')\n",
    "print(f'Clean CSVs   : {clean_dir}')\n",
    "print(f'Clean images : {clean_img_dir}')\n",
    "\n",
    "# --- OCR helpers ---\n",
    "def extract_text_from_image(image_bytes):\n",
    "    \"\"\"Extract text from image bytes using EasyOCR.\"\"\"\n",
    "    image   = Image.open(io.BytesIO(image_bytes))\n",
    "    results = reader_ocr.readtext(np.array(image), detail=1)\n",
    "    return image, results\n",
    "\n",
    "def boxes_are_adjacent(bbox1, bbox2, max_x_distance=30, max_y_distance=10):\n",
    "    \"\"\"Check if two bounding boxes are close enough to be the same card name.\"\"\"\n",
    "    x1_min = min(p[0] for p in bbox1); x1_max = max(p[0] for p in bbox1)\n",
    "    y1_min = min(p[1] for p in bbox1); y1_max = max(p[1] for p in bbox1)\n",
    "    x2_min = min(p[0] for p in bbox2); x2_max = max(p[0] for p in bbox2)\n",
    "    y2_min = min(p[1] for p in bbox2); y2_max = max(p[1] for p in bbox2)\n",
    "    y_overlap      = not (y1_max < y2_min - max_y_distance or y2_max < y1_min - max_y_distance)\n",
    "    horizontal_gap = min(abs(x1_max - x2_min), abs(x2_max - x1_min))\n",
    "    return y_overlap and horizontal_gap <= max_x_distance\n",
    "\n",
    "def merge_bboxes(bboxes):\n",
    "    \"\"\"Merge multiple bounding boxes into one encompassing box.\"\"\"\n",
    "    all_x = [p[0] for bbox in bboxes for p in bbox]\n",
    "    all_y = [p[1] for bbox in bboxes for p in bbox]\n",
    "    return [(min(all_x), min(all_y)), (max(all_x), min(all_y)),\n",
    "            (max(all_x), max(all_y)), (min(all_x), max(all_y))]\n",
    "\n",
    "def should_keep_text(text):\n",
    "    \"\"\"Filter out noise: short strings, mana symbols, UI labels, etc.\"\"\"\n",
    "    if len(text) < 3 or len(text) > 50:  return False\n",
    "    if not any(c.isalpha() for c in text): return False\n",
    "    if text.lower() in ['tap', 'untap', 'mana', 'cost', 'main', 'deck', 'sideboard']: return False\n",
    "    if all(c.isdigit() or c in '{}/WUBRGC' for c in text): return False\n",
    "    return True\n",
    "\n",
    "def parse_and_merge_card_names(ocr_results):\n",
    "    \"\"\"Group adjacent OCR detections into single card names, sorted top to bottom.\"\"\"\n",
    "    filtered = []\n",
    "    for bbox, text, confidence in ocr_results:\n",
    "        if confidence < 0.05: continue\n",
    "        text = text.strip()\n",
    "        if not should_keep_text(text): continue\n",
    "        filtered.append({\n",
    "            'bbox': bbox, 'text': text, 'confidence': confidence,\n",
    "            'x_min': min(p[0] for p in bbox), 'y_position': bbox[0][1]\n",
    "        })\n",
    "\n",
    "    merged_cards = []\n",
    "    used = set()\n",
    "    for i, det in enumerate(filtered):\n",
    "        if i in used: continue\n",
    "        group = [det]; used.add(i)\n",
    "        changed = True\n",
    "        while changed:\n",
    "            changed = False\n",
    "            for j, other in enumerate(filtered):\n",
    "                if j in used: continue\n",
    "                if any(boxes_are_adjacent(g['bbox'], other['bbox']) for g in group):\n",
    "                    group.append(other); used.add(j); changed = True; break\n",
    "        group.sort(key=lambda x: x['x_min'])\n",
    "        merged_cards.append({\n",
    "            'text':       ' '.join(d['text'] for d in group),\n",
    "            'confidence': sum(d['confidence'] for d in group) / len(group),\n",
    "            'bbox':       merge_bboxes([d['bbox'] for d in group]),\n",
    "            'y_position': group[0]['y_position']\n",
    "        })\n",
    "\n",
    "    merged_cards.sort(key=lambda x: x['y_position'])\n",
    "    return merged_cards\n",
    "\n",
    "# --- Card validation ---\n",
    "def validate_card(ocr_text, seen):\n",
    "    \"\"\"Match a single OCR result against the official card list.\n",
    "    Returns (status, official_name). Status: exact | exact_corrected | fuzzy | duplicate | unmatched.\n",
    "    `seen` is a set of already-used official names for duplicate detection.\n",
    "    \"\"\"\n",
    "    if ocr_text.lower() in official_cards_lower:\n",
    "        official_name = official_cards_lower[ocr_text.lower()]\n",
    "        if official_name in seen:\n",
    "            return 'duplicate', official_name\n",
    "        seen.add(official_name)\n",
    "        return ('exact' if ocr_text == official_name else 'exact_corrected'), official_name\n",
    "    matches = get_close_matches(ocr_text, official_cards, n=1, cutoff=SIMILARITY_THRESHOLD)\n",
    "    if matches:\n",
    "        official_name = matches[0]\n",
    "        if official_name in seen:\n",
    "            return 'duplicate', official_name\n",
    "        seen.add(official_name)\n",
    "        return 'fuzzy', official_name\n",
    "    return 'unmatched', None\n",
    "\n",
    "# --- Image drawing ---\n",
    "def draw_colored_boxes(image, merged_cards):\n",
    "    \"\"\"Draw green boxes for matched cards and red boxes for unmatched cards.\"\"\"\n",
    "    img_out = image.copy()\n",
    "    draw    = ImageDraw.Draw(img_out)\n",
    "    for card in merged_cards:\n",
    "        color = 'green' if card['status'] in ('exact', 'exact_corrected', 'fuzzy') else 'red'\n",
    "        draw.polygon(card['bbox'], outline=color, width=6)\n",
    "    return img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2c3d4e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T19:08:24.681216Z",
     "iopub.status.busy": "2026-02-22T19:08:24.681216Z",
     "iopub.status.idle": "2026-02-22T19:08:48.007519Z",
     "shell.execute_reply": "2026-02-22T19:08:48.007519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 12 player(s)...\n",
      "------------------------------------------------------------\n",
      "\n",
      "[1/12] Andrin\n",
      "  -> Downloading...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Running OCR...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 28 detections: 24 exact, 4 corrected, 0 unmatched, 0 duplicates\n",
      "  annotated_Andrin.jpeg\n",
      "  detailed_Andrin.csv\n",
      "  clean_Andrin.csv\n",
      "\n",
      "[2/12] Dimlas\n",
      "  -> Downloading...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Running OCR...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 36 detections: 26 exact, 9 corrected, 1 unmatched, 0 duplicates\n",
      "  annotated_Dimlas.jpeg\n",
      "  detailed_Dimlas.csv\n",
      "  clean_Dimlas.csv\n",
      "\n",
      "[3/12] Guy\n",
      "  -> Downloading...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Running OCR...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 38 detections: 28 exact, 5 corrected, 5 unmatched, 0 duplicates\n",
      "  annotated_Guy.jpeg\n",
      "  detailed_Guy.csv\n",
      "  clean_Guy.csv\n",
      "\n",
      "[4/12] Joel K.\n",
      "  -> Downloading...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Running OCR...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 25 detections: 20 exact, 5 corrected, 0 unmatched, 0 duplicates\n",
      "  annotated_Joel K..jpeg\n",
      "  detailed_Joel K..csv\n",
      "  clean_Joel K..csv\n",
      "\n",
      "[5/12] Kas\n",
      "  -> Downloading...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Running OCR...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 43 detections: 25 exact, 5 corrected, 13 unmatched, 0 duplicates\n",
      "  annotated_Kas.jpeg\n",
      "  detailed_Kas.csv\n",
      "  clean_Kas.csv\n",
      "\n",
      "[6/12] Lukas Stalder\n",
      "  -> Downloading...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Running OCR...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 42 detections: 24 exact, 6 corrected, 12 unmatched, 0 duplicates\n",
      "  annotated_Lukas Stalder.jpeg\n",
      "  detailed_Lukas Stalder.csv\n",
      "  clean_Lukas Stalder.csv\n",
      "\n",
      "[7/12] Myri\n",
      "  -> Downloading...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Running OCR...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 32 detections: 19 exact, 9 corrected, 3 unmatched, 1 duplicates\n",
      "  annotated_Myri.jpeg\n",
      "  detailed_Myri.csv\n",
      "  clean_Myri.csv\n",
      "\n",
      "[8/12] M채xu\n",
      "  -> Downloading...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Running OCR...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 34 detections: 31 exact, 2 corrected, 1 unmatched, 0 duplicates\n",
      "  annotated_M채xu.jpeg\n",
      "  detailed_M채xu.csv\n",
      "  clean_M채xu.csv\n",
      "\n",
      "[9/12] Noe T.\n",
      "  -> Downloading...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Running OCR...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 42 detections: 25 exact, 3 corrected, 14 unmatched, 0 duplicates\n",
      "  annotated_Noe T..jpeg\n",
      "  detailed_Noe T..csv\n",
      "  clean_Noe T..csv\n",
      "\n",
      "[10/12] Thomas\n",
      "  -> Downloading...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Running OCR...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 33 detections: 23 exact, 5 corrected, 5 unmatched, 0 duplicates\n",
      "  annotated_Thomas.jpeg\n",
      "  detailed_Thomas.csv\n",
      "  clean_Thomas.csv\n",
      "\n",
      "[11/12] Tinu\n",
      "  -> Downloading...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Running OCR...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 31 detections: 22 exact, 5 corrected, 4 unmatched, 0 duplicates\n",
      "  annotated_Tinu.jpeg\n",
      "  detailed_Tinu.csv\n",
      "  clean_Tinu.csv\n",
      "\n",
      "[12/12] Tommy\n",
      "  -> Downloading...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Running OCR...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 28 detections: 21 exact, 6 corrected, 1 unmatched, 0 duplicates\n",
      "  annotated_Tommy.jpeg\n",
      "  detailed_Tommy.csv\n",
      "  clean_Tommy.csv\n",
      "\n",
      "============================================================\n",
      "Done!\n",
      "  Clean images  -> c:\\Users\\Dimlas\\Desktop\\Dimi\\Github\\CubeOCR\\data\\clean\\20260222_Draft_8\\clean images\n",
      "  Clean CSVs    -> c:\\Users\\Dimlas\\Desktop\\Dimi\\Github\\CubeOCR\\data\\clean\\20260222_Draft_8\n",
      "  Detailed CSVs -> c:\\Users\\Dimlas\\Desktop\\Dimi\\Github\\CubeOCR\\data\\drafted_decks\\20260222_Draft_8\\detailed OCR\n"
     ]
    }
   ],
   "source": [
    "# --- Main processing loop ---\n",
    "print(f'Processing {len(player_files)} player(s)...')\n",
    "print('-' * 60)\n",
    "\n",
    "for idx, file in enumerate(player_files, 1):\n",
    "    player_name = os.path.splitext(file['name'])[0]\n",
    "    print(f'\\n[{idx}/{len(player_files)}] {player_name}')\n",
    "\n",
    "    try:\n",
    "        print('  -> Downloading...')\n",
    "        image_bytes = download_image(file['id'])\n",
    "\n",
    "        print('  -> Running OCR...')\n",
    "        original_image, ocr_results = extract_text_from_image(image_bytes)\n",
    "        merged_cards = parse_and_merge_card_names(ocr_results)\n",
    "\n",
    "        # Validate each detected card against the official list\n",
    "        seen = set()\n",
    "        for card in merged_cards:\n",
    "            status, official_name     = validate_card(card['text'], seen)\n",
    "            card['status']            = status\n",
    "            card['official_name']     = official_name\n",
    "\n",
    "        n_exact     = sum(1 for c in merged_cards if c['status'] in ('exact', 'exact_corrected'))\n",
    "        n_corrected = sum(1 for c in merged_cards if c['status'] == 'fuzzy')\n",
    "        n_unmatched = sum(1 for c in merged_cards if c['status'] == 'unmatched')\n",
    "        n_duplicate = sum(1 for c in merged_cards if c['status'] == 'duplicate')\n",
    "        print(f'  -> {len(merged_cards)} detections: {n_exact} exact, {n_corrected} corrected, {n_unmatched} unmatched, {n_duplicate} duplicates')\n",
    "\n",
    "        # Save color-coded annotated image -> data/clean/{draft}/clean images/\n",
    "        colored_image = draw_colored_boxes(original_image, merged_cards)\n",
    "        img_path = os.path.join(clean_img_dir, f'annotated_{player_name}.jpeg')\n",
    "        colored_image.save(img_path, quality=90)\n",
    "\n",
    "        # Save raw OCR CSV (unvalidated) -> data/drafted_decks/{draft}/\n",
    "        raw_csv_path = os.path.join(output_dir, f'{player_name}.csv')\n",
    "        with open(raw_csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['name'])\n",
    "            for card in merged_cards:\n",
    "                writer.writerow([card['text']])\n",
    "\n",
    "        # Save detailed validation CSV -> data/drafted_decks/{draft}/detailed OCR/\n",
    "        detailed_path = os.path.join(detailed_dir, f'detailed_{player_name}.csv')\n",
    "        with open(detailed_path, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['status', 'official_name', 'ocr_input', 'note'])\n",
    "            for card in merged_cards:\n",
    "                s     = card['status']\n",
    "                oname = card['official_name'] or ''\n",
    "                ocr   = card['text']\n",
    "                if s in ('exact', 'exact_corrected'):\n",
    "                    writer.writerow(['exact',     oname, ocr, ''])\n",
    "                elif s == 'fuzzy':\n",
    "                    writer.writerow(['corrected', oname, ocr, f'corrected from: {ocr}'])\n",
    "                elif s == 'unmatched':\n",
    "                    writer.writerow(['unmatched', '',    ocr, 'no match found'])\n",
    "                elif s == 'duplicate':\n",
    "                    writer.writerow(['duplicate', oname, ocr, 'duplicate removed'])\n",
    "\n",
    "        # Save clean deck list -> data/clean/{draft}/\n",
    "        clean_csv_path = os.path.join(clean_dir, f'clean_{player_name}.csv')\n",
    "        with open(clean_csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['name', 'scryfall_id'])\n",
    "            for card in merged_cards:\n",
    "                if card['status'] in ('exact', 'exact_corrected', 'fuzzy'):\n",
    "                    writer.writerow([card['official_name'], name_to_scryfall_id.get(card['official_name'], '')])\n",
    "\n",
    "        print(f'  annotated_{player_name}.jpeg')\n",
    "        print(f'  detailed_{player_name}.csv')\n",
    "        print(f'  clean_{player_name}.csv')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'  ERROR: {e}')\n",
    "\n",
    "print(f'\\n{\"=\" * 60}')\n",
    "print(f'Done!')\n",
    "print(f'  Clean images  -> {clean_img_dir}')\n",
    "print(f'  Clean CSVs    -> {clean_dir}')\n",
    "print(f'  Detailed CSVs -> {detailed_dir}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
