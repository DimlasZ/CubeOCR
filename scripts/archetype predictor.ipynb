{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "archetype-predictor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using draft folder: 20260125_Draft_7\n",
      "Loaded 1,334 archetype rows | 580 unique cards\n"
     ]
    }
   ],
   "source": [
    "# Archetype Predictor\n",
    "# Predicts the archetype of each drafted deck by comparing card scryfall IDs\n",
    "# against the cleaned archetype win rate data.\n",
    "#\n",
    "# How it works:\n",
    "#   For each card in the deck, look up which archetypes it appears in.\n",
    "#   Each archetype gets a score equal to the sum of games_played for that card.\n",
    "#   The archetype with the highest total score is the prediction.\n",
    "#\n",
    "# Input:  data/archetype_decktype_data/archetype_data.csv\n",
    "#         data/clean/<draft_folder>/clean_<player>.csv\n",
    "# Output: predicted archetype per player (printed as a summary table)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Project root is one level up from this scripts/ folder\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "ARCHETYPE_FILE = os.path.join(PROJECT_ROOT, 'data', 'archetype_decktype_data', 'archetype_data.csv')\n",
    "\n",
    "# Auto-detect the newest draft folder\n",
    "CLEAN_ROOT = os.path.join(PROJECT_ROOT, 'data', 'clean')\n",
    "draft_folders = sorted([\n",
    "    d for d in os.listdir(CLEAN_ROOT)\n",
    "    if os.path.isdir(os.path.join(CLEAN_ROOT, d))\n",
    "])\n",
    "DRAFT_FOLDER = draft_folders[-1]\n",
    "CLEAN_DIR = os.path.join(CLEAN_ROOT, DRAFT_FOLDER)\n",
    "print(f\"Using draft folder: {DRAFT_FOLDER}\")\n",
    "\n",
    "ARCHETYPES = [\n",
    "    'Aggro',\n",
    "    'Aggro-Combo',\n",
    "    'Aggro-Control (Tempo)',\n",
    "    'Combo',\n",
    "    'Combo-Control',\n",
    "    'Control',\n",
    "    'Control-Aggro (Midrange)',\n",
    "]\n",
    "\n",
    "# Load archetype data, build lookup: scryfallId -> {archetype: games_played}\n",
    "archetype_df = pd.read_csv(ARCHETYPE_FILE)\n",
    "card_scores = (\n",
    "    archetype_df\n",
    "    .groupby(['scryfallId', 'archetype'])['games_played']\n",
    "    .sum()\n",
    ")\n",
    "print(f\"Loaded {len(archetype_df):,} archetype rows | {archetype_df['scryfallId'].nunique()} unique cards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "predict-archetypes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 deck(s) in 20260125_Draft_7\n",
      "\n",
      "Andrin               -> Control\n",
      "  matched: 26/28 cards\n",
      "  unmatched: ['Kaito, Bane of Nightmares', \"Bloodchief's Thirst\"]\n",
      "  scores (%): {'Aggro': 10.6, 'Aggro-Combo': 0.0, 'Aggro-Control (Tempo)': 17.1, 'Combo': 31.6, 'Combo-Control': 4.7, 'Control': 33.6, 'Control-Aggro (Midrange)': 2.6}\n",
      "\n",
      "Dimlas               -> Aggro\n",
      "  matched: 26/27 cards\n",
      "  unmatched: ['Enduring Innocence']\n",
      "  scores (%): {'Aggro': 60.2, 'Aggro-Combo': 0.0, 'Aggro-Control (Tempo)': 2.9, 'Combo': 8.0, 'Combo-Control': 4.0, 'Control': 25.0, 'Control-Aggro (Midrange)': 0.0}\n",
      "\n",
      "Fubu                 -> Aggro\n",
      "  matched: 22/27 cards\n",
      "  unmatched: ['Baloth Prime', 'Greasewrench Goblin', 'Badgermole Cub', 'Shadowspear', 'Formidable Speaker']\n",
      "  scores (%): {'Aggro': 26.2, 'Aggro-Combo': 24.3, 'Aggro-Control (Tempo)': 0.0, 'Combo': 15.8, 'Combo-Control': 5.2, 'Control': 9.6, 'Control-Aggro (Midrange)': 18.9}\n",
      "\n",
      "Joel K.              -> Combo\n",
      "  matched: 19/22 cards\n",
      "  unmatched: ['Murktide Regent', 'Infernal Grasp', 'Wan Shi Tong, Librarian']\n",
      "  scores (%): {'Aggro': 3.2, 'Aggro-Combo': 2.3, 'Aggro-Control (Tempo)': 3.2, 'Combo': 44.7, 'Combo-Control': 5.5, 'Control': 37.9, 'Control-Aggro (Midrange)': 3.2}\n",
      "\n",
      "Lukas Stalder        -> Control\n",
      "  matched: 28/30 cards\n",
      "  unmatched: ['Sleight of Hand', 'Wan Shi Tong, All-Knowing']\n",
      "  scores (%): {'Aggro': 21.9, 'Aggro-Combo': 0.6, 'Aggro-Control (Tempo)': 5.1, 'Combo': 13.7, 'Combo-Control': 6.8, 'Control': 49.3, 'Control-Aggro (Midrange)': 2.6}\n",
      "\n",
      "Matthias             -> Combo\n",
      "  matched: 29/29 cards\n",
      "  scores (%): {'Aggro': 31.1, 'Aggro-Combo': 0.0, 'Aggro-Control (Tempo)': 2.2, 'Combo': 44.2, 'Combo-Control': 4.4, 'Control': 16.1, 'Control-Aggro (Midrange)': 2.1}\n",
      "\n",
      "Noe T.               -> Aggro\n",
      "  matched: 27/27 cards\n",
      "  scores (%): {'Aggro': 49.1, 'Aggro-Combo': 5.6, 'Aggro-Control (Tempo)': 2.5, 'Combo': 28.9, 'Combo-Control': 4.3, 'Control': 8.0, 'Control-Aggro (Midrange)': 1.6}\n",
      "\n",
      "Sili                 -> Combo\n",
      "  matched: 27/27 cards\n",
      "  scores (%): {'Aggro': 8.1, 'Aggro-Combo': 0.0, 'Aggro-Control (Tempo)': 4.0, 'Combo': 54.3, 'Combo-Control': 7.0, 'Control': 24.7, 'Control-Aggro (Midrange)': 1.9}\n",
      "\n",
      "Tinu                 -> Control-Aggro (Midrange)\n",
      "  matched: 33/35 cards\n",
      "  unmatched: [\"Esika's Chariot\", 'Opposition']\n",
      "  scores (%): {'Aggro': 21.8, 'Aggro-Combo': 11.1, 'Aggro-Control (Tempo)': 1.0, 'Combo': 12.2, 'Combo-Control': 7.4, 'Control': 21.5, 'Control-Aggro (Midrange)': 25.2}\n",
      "\n",
      "Tommy                -> Combo\n",
      "  matched: 36/36 cards\n",
      "  scores (%): {'Aggro': 13.9, 'Aggro-Combo': 0.8, 'Aggro-Control (Tempo)': 3.0, 'Combo': 35.2, 'Combo-Control': 15.5, 'Control': 28.6, 'Control-Aggro (Midrange)': 3.2}\n",
      "\n",
      "Valentin             -> Combo\n",
      "  matched: 28/30 cards\n",
      "  unmatched: ['Tezzeret, Cruel Captain', 'The Tabernacle at Pendrell Vale']\n",
      "  scores (%): {'Aggro': 19.3, 'Aggro-Combo': 0.0, 'Aggro-Control (Tempo)': 1.7, 'Combo': 38.5, 'Combo-Control': 4.1, 'Control': 32.5, 'Control-Aggro (Midrange)': 4.0}\n",
      "\n",
      "Yannik               -> Control-Aggro (Midrange)\n",
      "  matched: 25/27 cards\n",
      "  unmatched: ['Ouroboroid', 'Abrupt Decay']\n",
      "  scores (%): {'Aggro': 12.8, 'Aggro-Combo': 17.6, 'Aggro-Control (Tempo)': 1.8, 'Combo': 20.1, 'Combo-Control': 8.3, 'Control': 14.0, 'Control-Aggro (Midrange)': 25.4}\n",
      "\n",
      "\n",
      "--- Summary ---\n",
      "       player      predicted_archetype  cards_matched  cards_unmatched\n",
      "       Andrin                  Control             26                2\n",
      "       Dimlas                    Aggro             26                1\n",
      "         Fubu                    Aggro             22                5\n",
      "      Joel K.                    Combo             19                3\n",
      "Lukas Stalder                  Control             28                2\n",
      "     Matthias                    Combo             29                0\n",
      "       Noe T.                    Aggro             27                0\n",
      "         Sili                    Combo             27                0\n",
      "         Tinu Control-Aggro (Midrange)             33                2\n",
      "        Tommy                    Combo             36                0\n",
      "     Valentin                    Combo             28                2\n",
      "       Yannik Control-Aggro (Midrange)             25                2\n"
     ]
    }
   ],
   "source": [
    "# Predict archetype for each clean deck\n",
    "\n",
    "results = []\n",
    "clean_files = sorted(glob.glob(os.path.join(CLEAN_DIR, 'clean_*.csv')))\n",
    "print(f\"Found {len(clean_files)} deck(s) in {DRAFT_FOLDER}\\n\")\n",
    "\n",
    "for filepath in clean_files:\n",
    "    player = os.path.basename(filepath).replace('clean_', '').replace('.csv', '')\n",
    "\n",
    "    deck_df = pd.read_csv(filepath)\n",
    "    scryfall_ids = deck_df['scryfall_id'].dropna().str.strip().tolist()\n",
    "\n",
    "    # Sum games_played per archetype across all cards in the deck\n",
    "    scores = {arch: 0 for arch in ARCHETYPES}\n",
    "    matched = 0\n",
    "    unmatched = []\n",
    "\n",
    "    for sid in scryfall_ids:\n",
    "        if sid in card_scores.index.get_level_values(0):\n",
    "            matched += 1\n",
    "            for arch, gp in card_scores[sid].items():\n",
    "                if arch in scores:\n",
    "                    scores[arch] += gp\n",
    "        else:\n",
    "            card_name = deck_df.loc[deck_df['scryfall_id'] == sid, 'name'].values\n",
    "            unmatched.append(card_name[0] if len(card_name) > 0 else sid)\n",
    "\n",
    "    predicted = max(scores, key=scores.get)\n",
    "    total_score = sum(scores.values())\n",
    "\n",
    "    breakdown = {arch: round(v / total_score * 100, 1) if total_score > 0 else 0\n",
    "                 for arch, v in scores.items()}\n",
    "\n",
    "    results.append({\n",
    "        'player': player,\n",
    "        'predicted_archetype': predicted,\n",
    "        'cards_matched': matched,\n",
    "        'cards_unmatched': len(unmatched),\n",
    "        **{f'score_{a}': breakdown[a] for a in ARCHETYPES},\n",
    "    })\n",
    "\n",
    "    print(f\"{player:20s} -> {predicted}\")\n",
    "    print(f\"  matched: {matched}/{len(scryfall_ids)} cards\")\n",
    "    if unmatched:\n",
    "        print(f\"  unmatched: {unmatched}\")\n",
    "    print(f\"  scores (%): { {a: breakdown[a] for a in ARCHETYPES} }\")\n",
    "    print()\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(results_df[['player', 'predicted_archetype', 'cards_matched', 'cards_unmatched']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wdvpfia158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: 2026_01_25_drafted_decks.csv\n",
      "\n",
      "Player               Truth                          Raw Sum                        Normalized                      Raw  Norm\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "Andrin               Aggro-Control (Tempo)          Control                        Control                           X     X\n",
      "Dimlas               Aggro                          Aggro                          Aggro                            OK    OK\n",
      "Fubu                 Aggro-Combo                    Aggro                          Aggro                             X     X\n",
      "Joel K.              Control                        Combo                          Control                           X    OK\n",
      "Lukas Stalder        Control                        Control                        Control                          OK    OK\n",
      "Matthias             Combo                          Combo                          Combo                            OK    OK\n",
      "Noe T.               Aggro-Combo                    Aggro                          Aggro                             X     X\n",
      "Sili                 Combo                          Combo                          Combo                            OK    OK\n",
      "Tinu                 Control-Aggro (Midrange)       Control-Aggro (Midrange)       Control-Aggro (Midrange)         OK    OK\n",
      "Tommy                Combo-Control                  Combo                          Combo                             X     X\n",
      "Valentin             Control                        Combo                          Combo                             X     X\n",
      "Yannik               Control-Aggro (Midrange)       Control-Aggro (Midrange)       Control-Aggro (Midrange)         OK    OK\n",
      "\n",
      "Accuracy — Raw Sum: 50% (6/12)  Normalized: 58% (7/12)\n",
      "Both correct: 6  Only Raw: 0  Only Norm: 1\n",
      "Winner: Normalized\n"
     ]
    }
   ],
   "source": [
    "# Method comparison: Raw Sum vs Normalized Per-Card\n",
    "#\n",
    "# Raw Sum (current):    score(arch) = sum of games_played(card, arch) across all cards\n",
    "#                       Cards with more historical data pull harder.\n",
    "#\n",
    "# Normalized Per-Card:  score(arch) = sum of [games_played(card, arch) / total_games(card)]\n",
    "#                       Each card contributes equally regardless of data volume.\n",
    "#\n",
    "# Ground truth loaded from data/archetype_decktype_data/<date>_drafted_decks.csv\n",
    "# (auto-detects the most recent file matching that pattern)\n",
    "\n",
    "import glob as _glob\n",
    "\n",
    "# --- Load ground truth ---\n",
    "gt_pattern = os.path.join(PROJECT_ROOT, 'data', 'archetype_decktype_data', '*_drafted_decks.csv')\n",
    "gt_files = sorted(_glob.glob(gt_pattern))\n",
    "if not gt_files:\n",
    "    print(\"No ground truth file found - skipping comparison.\")\n",
    "else:\n",
    "    gt_file = gt_files[-1]\n",
    "    print(f\"Ground truth: {os.path.basename(gt_file)}\\n\")\n",
    "    gt_df = pd.read_csv(gt_file)\n",
    "    # One archetype per player (all rows for a player share the same archetype)\n",
    "    gt = gt_df.groupby('player')['archetype'].first().to_dict()\n",
    "\n",
    "    # --- Precompute total games per card for normalization ---\n",
    "    card_total_games = archetype_df.groupby('scryfallId')['games_played'].sum()\n",
    "\n",
    "    # --- Run both methods ---\n",
    "    comparison = []\n",
    "\n",
    "    for filepath in sorted(_glob.glob(os.path.join(CLEAN_DIR, 'clean_*.csv'))):\n",
    "        player = os.path.basename(filepath).replace('clean_', '').replace('.csv', '')\n",
    "        deck_df = pd.read_csv(filepath)\n",
    "        scryfall_ids = deck_df['scryfall_id'].dropna().str.strip().tolist()\n",
    "\n",
    "        raw_scores  = {arch: 0.0 for arch in ARCHETYPES}\n",
    "        norm_scores = {arch: 0.0 for arch in ARCHETYPES}\n",
    "\n",
    "        for sid in scryfall_ids:\n",
    "            if sid not in card_scores.index.get_level_values(0):\n",
    "                continue\n",
    "            card_archs = card_scores[sid]\n",
    "            total = card_total_games.get(sid, 1)\n",
    "            for arch, gp in card_archs.items():\n",
    "                if arch in ARCHETYPES:\n",
    "                    raw_scores[arch]  += gp\n",
    "                    norm_scores[arch] += gp / total\n",
    "\n",
    "        pred_raw  = max(raw_scores,  key=raw_scores.get)\n",
    "        pred_norm = max(norm_scores, key=norm_scores.get)\n",
    "        truth = gt.get(player, '?')\n",
    "\n",
    "        comparison.append({\n",
    "            'player':     player,\n",
    "            'truth':      truth,\n",
    "            'raw_sum':    pred_raw,\n",
    "            'normalized': pred_norm,\n",
    "            'raw_ok':     pred_raw  == truth,\n",
    "            'norm_ok':    pred_norm == truth,\n",
    "        })\n",
    "\n",
    "    cmp_df = pd.DataFrame(comparison)\n",
    "\n",
    "    # --- Print results ---\n",
    "    print(f\"{'Player':<20} {'Truth':<30} {'Raw Sum':<30} {'Normalized':<30} {'Raw':>4} {'Norm':>5}\")\n",
    "    print('-' * 115)\n",
    "    for _, row in cmp_df.iterrows():\n",
    "        print(f\"{row['player']:<20} {row['truth']:<30} {row['raw_sum']:<30} {row['normalized']:<30} \"\n",
    "              f\"{'OK' if row['raw_ok'] else 'X':>4} {'OK' if row['norm_ok'] else 'X':>5}\")\n",
    "\n",
    "    raw_acc  = cmp_df['raw_ok'].mean()\n",
    "    norm_acc = cmp_df['norm_ok'].mean()\n",
    "    both_right = (cmp_df['raw_ok'] & cmp_df['norm_ok']).sum()\n",
    "    only_raw   = (cmp_df['raw_ok'] & ~cmp_df['norm_ok']).sum()\n",
    "    only_norm  = (~cmp_df['raw_ok'] & cmp_df['norm_ok']).sum()\n",
    "\n",
    "    print(f\"\\nAccuracy — Raw Sum: {raw_acc:.0%} ({cmp_df['raw_ok'].sum()}/{len(cmp_df)})  \"\n",
    "          f\"Normalized: {norm_acc:.0%} ({cmp_df['norm_ok'].sum()}/{len(cmp_df)})\")\n",
    "    print(f\"Both correct: {both_right}  Only Raw: {only_raw}  Only Norm: {only_norm}\")\n",
    "    winner = 'Normalized' if norm_acc > raw_acc else ('Raw Sum' if raw_acc > norm_acc else 'Tie')\n",
    "    print(f\"Winner: {winner}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
